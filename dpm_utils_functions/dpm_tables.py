# __________________________________________________________________________________________________________________________________________________________
# Repositorio de funciones
# __________________________________________________________________________________________________________________________________________________________
from google.cloud import bigquery
import pandas as pd
import pandas_gbq

from dpm_common_functions import _ini_authenticate_API

import unicodedata
import re
import time
import os
import io
import json
from typing import Dict, Any



# ----------------------------------------------------------------------------
# fields_name_format()
# ----------------------------------------------------------------------------
def fields_name_format(config):
    """
    Formatea nombres de campos de datos seg√∫n configuraciones espec√≠ficas.
    
    Par√°metros en config:
      - fields_name_raw_list (list): Lista de nombres de campos.
      - formato_final (str, opcional): 'CamelCase', 'snake_case', 'Sentence case', o None.
      - reemplazos (dict, opcional): Diccionario de t√©rminos a reemplazar.
      - siglas (list, opcional): Lista de siglas que deben mantenerse en may√∫sculas.
    
    Retorna:
        pd.DataFrame: DataFrame con columnas 'Campo Original' y 'Campo Formateado'.
    """
    print("[START üöÄ] Iniciando formateo de nombres de campos...", flush=True)
    
    def aplicar_reemplazos(field, reemplazos):
        for key, value in sorted(reemplazos.items(), key=lambda x: -len(x[0])):
            if key in field:
                field = field.replace(key, value)
        return field

    def formatear_campo(field, formato, siglas):
        if formato is None or formato is False:
            return field
        words = [w for w in re.split(r'[_\-\s]+', field) if w]
        if formato == 'CamelCase':
            return ''.join(
                word.upper() if word.upper() in siglas
                else word.capitalize() if idx == 0
                else word.lower()
                for idx, word in enumerate(words)
            )
        elif formato == 'snake_case':
            return '_'.join(
                word.upper() if word.upper() in siglas
                else word.lower() for word in words
            )
        elif formato == 'Sentence case':
            return ' '.join(
                word.upper() if word.upper() in siglas
                else word.capitalize() if idx == 0
                else word.lower()
                for idx, word in enumerate(words)
            )
        else:
            raise ValueError(f"Formato '{formato}' no soportado.")
    
    resultado = []
    for field in config.get('fields_name_raw_list', []):
        original_field = field
        field = aplicar_reemplazos(field, config.get('reemplazos', {}))
        formatted_field = formatear_campo(field, config.get('formato_final', 'CamelCase'), [sig.upper() for sig in config.get('siglas', [])])
        resultado.append({'Campo Original': original_field, 'Campo Formateado': formatted_field})
    
    df_result = pd.DataFrame(resultado)
    print("[END [FINISHED üèÅ]] Formateo de nombres completado.\n", flush=True)
    return df_result









# ----------------------------------------------------------------------------
# tables_consolidate_duplicates_df()
# ----------------------------------------------------------------------------
def tables_consolidate_duplicates_df(config_dic: dict):
    """Consolida dos ``pandas.DataFrame`` resolviendo duplicados.

    La funci√≥n primero limpia duplicados internos en cada DataFrame, luego
    aplica la pol√≠tica indicada en *config_dic* y devuelve un ``pd.DataFrame``
    limpio (y metadatos opcionales) siguiendo el Manual de Estilo DPM.

    Args:
        config_dic (dict):
            validate_df_schemas_match (bool, opcional): Valida coincidencia de
                columnas entre DataFrames (default ``True``).
            df_initial (pd.DataFrame): DataFrame fuente prioritario.
            df_to_merge (pd.DataFrame): DataFrame a fusionar.
            id_fields (list[str]): Campos clave para identificar registros.
            duplicate_policy (str): ``keep_newest`` | ``keep_oldest`` |
                ``keep_df_initial`` | ``keep_df_to_merge``.
            duplicate_date_field (str, opcional): Campo fecha para pol√≠ticas
                basadas en tiempo.
            duplicate_date_field_format_str (str, opcional): Formato
                ``datetime.strptime`` de *duplicate_date_field*.
            return_metadata (bool, opcional): Si ``True`` devuelve metadatos.

    Returns:
        pd.DataFrame | tuple[pd.DataFrame, dict]: DataFrame consolidado y,
        opcionalmente, metadatos del proceso.

    Raises:
        ValueError: Par√°metros err√≥neos o esquemas distintos.
        TypeError: *df_initial* o *df_to_merge* no son ``pd.DataFrame``.
    """

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Imports locales m√≠nimos
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    import pandas as pd  # type: ignore
    import numpy as np
    from datetime import datetime

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 1Ô∏è‚É£ Extracci√≥n y validaci√≥n de par√°metros
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    allowed_policies_set = {
        "keep_newest",
        "keep_oldest",
        "keep_df_initial",
        "keep_df_to_merge",
    }

    validate_schema_bool: bool = config_dic.get("validate_df_schemas_match", True)
    df_initial_df = config_dic.get("df_initial")
    df_to_merge_df = config_dic.get("df_to_merge")
    id_fields_list: list[str] = config_dic.get("id_fields", [])
    policy_str: str = config_dic.get("duplicate_policy", "keep_newest")
    date_col_str: str | None = config_dic.get("duplicate_date_field")
    date_fmt_str: str | None = config_dic.get("duplicate_date_field_format_str")
    return_meta_bool: bool = config_dic.get("return_metadata", False)

    print(f"[CONSOLIDATION START ‚ñ∂Ô∏è] {datetime.now().isoformat(timespec='seconds')}", flush=True)
    print(f"INFO ‚ÑπÔ∏è id_fields={id_fields_list} | policy={policy_str}", flush=True)

    # Validaciones b√°sicas
    if policy_str not in allowed_policies_set:
        raise ValueError(f"duplicate_policy debe ser uno de {allowed_policies_set}")

    if not isinstance(df_initial_df, pd.DataFrame) or not isinstance(df_to_merge_df, pd.DataFrame):
        raise TypeError("df_initial y df_to_merge deben ser DataFrame")

    if not id_fields_list:
        raise ValueError("id_fields no puede ser vac√≠o")

    for col_str in id_fields_list:
        if col_str not in df_initial_df.columns or col_str not in df_to_merge_df.columns:
            raise ValueError(f"Columna clave '{col_str}' ausente en alguno de los DataFrames")

    if policy_str in ("keep_newest", "keep_oldest"):
        if not date_col_str:
            raise ValueError("duplicate_date_field es obligatorio para la pol√≠tica basada en fecha")
        if (date_col_str not in df_initial_df.columns or date_col_str not in df_to_merge_df.columns):
            raise ValueError(f"Campo fecha '{date_col_str}' inexistente en ambos DataFrames")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 2Ô∏è‚É£ Validaci√≥n opcional de esquemas
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if validate_schema_bool and set(df_initial_df.columns) != set(df_to_merge_df.columns):
        diff_left_set = set(df_initial_df.columns) - set(df_to_merge_df.columns)
        diff_right_set = set(df_to_merge_df.columns) - set(df_initial_df.columns)
        print(f"[VALIDATION ERROR ‚ùå] Schemas difieren ‚Äì izquierda: {diff_left_set} | derecha: {diff_right_set}", flush=True)
        raise ValueError("Esquemas distintos entre DataFrames")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 3Ô∏è‚É£ PASO CR√çTICO: Limpiar duplicados internos primero
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print(f"DEBUG üîç Registros originales: df_initial={len(df_initial_df)}, df_to_merge={len(df_to_merge_df)}", flush=True)
    
    # Detectar duplicados internos
    initial_internal_dups = df_initial_df.duplicated(subset=id_fields_list).sum()
    merge_internal_dups = df_to_merge_df.duplicated(subset=id_fields_list).sum()
    
    print(f"DEBUG üîç Duplicados internos detectados: df_initial={initial_internal_dups}, df_to_merge={merge_internal_dups}", flush=True)
    
    # Limpiar duplicados internos usando la misma pol√≠tica
    if initial_internal_dups > 0:
        print(f"CLEANING üßπ Limpiando {initial_internal_dups} duplicados internos en df_initial", flush=True)
        df_initial_clean = _clean_internal_duplicates(df_initial_df, id_fields_list, date_col_str, date_fmt_str, policy_str)
    else:
        df_initial_clean = df_initial_df.copy()
    
    if merge_internal_dups > 0:
        print(f"CLEANING üßπ Limpiando {merge_internal_dups} duplicados internos en df_to_merge", flush=True)
        df_to_merge_clean = _clean_internal_duplicates(df_to_merge_df, id_fields_list, date_col_str, date_fmt_str, policy_str)
    else:
        df_to_merge_clean = df_to_merge_df.copy()
    
    print(f"DEBUG üîç Registros despu√©s de limpieza interna: df_initial={len(df_initial_clean)}, df_to_merge={len(df_to_merge_clean)}", flush=True)
    
    # Verificar que no quedan duplicados internos
    final_initial_dups = df_initial_clean.duplicated(subset=id_fields_list).sum()
    final_merge_dups = df_to_merge_clean.duplicated(subset=id_fields_list).sum()
    
    if final_initial_dups > 0 or final_merge_dups > 0:
        print(f"[ERROR ‚ùå] A√∫n quedan duplicados internos: df_initial={final_initial_dups}, df_to_merge={final_merge_dups}", flush=True)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 4Ô∏è‚É£ Limpiar espacios en id_fields
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    for field in id_fields_list:
        if df_initial_clean[field].dtype == 'object':
            df_initial_clean[field] = df_initial_clean[field].astype(str).str.strip()
        if df_to_merge_clean[field].dtype == 'object':
            df_to_merge_clean[field] = df_to_merge_clean[field].astype(str).str.strip()

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 5Ô∏è‚É£ Identificar registros para consolidaci√≥n
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    # Crear identificador √∫nico para cada registro
    df_initial_clean['_temp_id'] = df_initial_clean[id_fields_list].apply(lambda x: '||'.join(x.astype(str)), axis=1)
    df_to_merge_clean['_temp_id'] = df_to_merge_clean[id_fields_list].apply(lambda x: '||'.join(x.astype(str)), axis=1)
    
    # Identificar qu√© registros est√°n en ambos DataFrames
    ids_initial = set(df_initial_clean['_temp_id'])
    ids_to_merge = set(df_to_merge_clean['_temp_id'])
    
    common_ids = ids_initial.intersection(ids_to_merge)
    only_initial = ids_initial - ids_to_merge
    only_to_merge = ids_to_merge - ids_initial
    
    print(f"DEBUG üîç An√°lisis de intersecci√≥n:", flush=True)
    print(f"  - Solo en df_initial: {len(only_initial)}", flush=True)
    print(f"  - Solo en df_to_merge: {len(only_to_merge)}", flush=True)
    print(f"  - En ambos (requieren consolidaci√≥n): {len(common_ids)}", flush=True)
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 6Ô∏è‚É£ Separar registros seg√∫n su situaci√≥n
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    # Registros √∫nicos (no requieren consolidaci√≥n)
    unique_from_initial = df_initial_clean[df_initial_clean['_temp_id'].isin(only_initial)].copy()
    unique_from_merge = df_to_merge_clean[df_to_merge_clean['_temp_id'].isin(only_to_merge)].copy()
    
    # Registros que requieren consolidaci√≥n
    conflicting_initial = df_initial_clean[df_initial_clean['_temp_id'].isin(common_ids)].copy()
    conflicting_merge = df_to_merge_clean[df_to_merge_clean['_temp_id'].isin(common_ids)].copy()
    
    print(f"DEBUG üîç Distribuci√≥n de registros:", flush=True)
    print(f"  - √önicos de initial: {len(unique_from_initial)}", flush=True)
    print(f"  - √önicos de merge: {len(unique_from_merge)}", flush=True)
    print(f"  - Conflictivos de initial: {len(conflicting_initial)}", flush=True)
    print(f"  - Conflictivos de merge: {len(conflicting_merge)}", flush=True)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 7Ô∏è‚É£ Resolver conflictos
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if len(common_ids) > 0:
        print(f"RESOLVING üîß Aplicando pol√≠tica '{policy_str}' a {len(common_ids)} registros conflictivos", flush=True)
        
        # Combinar registros conflictivos para resoluci√≥n
        conflicting_all = pd.concat([conflicting_initial, conflicting_merge], ignore_index=True)
        conflicting_all['_source'] = ['initial'] * len(conflicting_initial) + ['to_merge'] * len(conflicting_merge)
        
        resolved_conflicts = _resolve_conflicts(conflicting_all, id_fields_list, date_col_str, date_fmt_str, policy_str)
        
        print(f"DEBUG üîç Conflictos resueltos: {len(resolved_conflicts)} registros", flush=True)
    else:
        resolved_conflicts = pd.DataFrame(columns=df_initial_clean.columns)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 8Ô∏è‚É£ Combinar resultado final
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    # Limpiar columnas temporales
    for df in [unique_from_initial, unique_from_merge, resolved_conflicts]:
        if len(df) > 0:
            df.drop(columns=['_temp_id'], errors='ignore', inplace=True)
            df.drop(columns=['_source'], errors='ignore', inplace=True)
    
    # Combinar todos los registros
    result_parts = []
    if len(unique_from_initial) > 0:
        result_parts.append(unique_from_initial)
    if len(unique_from_merge) > 0:
        result_parts.append(unique_from_merge)
    if len(resolved_conflicts) > 0:
        result_parts.append(resolved_conflicts)
    
    if result_parts:
        result_df = pd.concat(result_parts, ignore_index=True)
    else:
        result_df = pd.DataFrame(columns=df_initial_df.columns)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # 9Ô∏è‚É£ Verificaci√≥n final exhaustiva
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    result_df.reset_index(drop=True, inplace=True)
    
    # Verificar duplicados finales
    final_duplicates_id = result_df.duplicated(subset=id_fields_list).sum()
    final_duplicates_exact = result_df.duplicated().sum()
    
    print(f"VERIFICATION üîç Verificaci√≥n final:", flush=True)
    print(f"  - Registros finales: {len(result_df)}", flush=True)
    print(f"  - Duplicados por id_fields: {final_duplicates_id}", flush=True)
    print(f"  - Duplicados exactos: {final_duplicates_exact}", flush=True)
    
    if final_duplicates_id > 0:
        print(f"[ERROR ‚ùå] CR√çTICO: A√öN QUEDAN {final_duplicates_id} DUPLICADOS POR ID_FIELDS", flush=True)
        # Mostrar ejemplos
        dup_examples = result_df[result_df.duplicated(subset=id_fields_list, keep=False)][id_fields_list].head(10)
        print(f"Ejemplos de duplicados:\n{dup_examples.to_string()}", flush=True)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # üîü Alinear dtypes y metadatos finales
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    for col_str, dtype in df_initial_df.dtypes.items():
        if col_str in result_df.columns:
            try:
                result_df[col_str] = result_df[col_str].astype(dtype, copy=False)
            except (ValueError, TypeError) as e:
                print(f"[TYPE WARN ‚ö†Ô∏è] No se pudo convertir '{col_str}' a {dtype}: {e}", flush=True)

    total_original = len(df_initial_df) + len(df_to_merge_df)
    duplicates_resolved = total_original - len(result_df)
    
    print(f"FINISHED ‚úÖ consolidaci√≥n completada:", flush=True)
    print(f"  - Registros originales totales: {total_original}", flush=True)
    print(f"  - Registros finales: {len(result_df)}", flush=True)
    print(f"  - Duplicados eliminados: {duplicates_resolved}", flush=True)
    print(f"  - Registros agregados al df_initial: {len(result_df) - len(df_initial_df)}", flush=True)

    if return_meta_bool:
        metadata_dic: dict = {
            "timestamp": datetime.now(),
            "initial_records": len(df_initial_df),
            "merge_records": len(df_to_merge_df),
            "initial_internal_duplicates": initial_internal_dups,
            "merge_internal_duplicates": merge_internal_dups,
            "records_only_initial": len(only_initial),
            "records_only_merge": len(only_to_merge),
            "records_in_both": len(common_ids),
            "final_records": len(result_df),
            "total_duplicates_resolved": duplicates_resolved,
            "records_added": len(result_df) - len(df_initial_df),
            "final_duplicates_by_id": final_duplicates_id,
            "final_duplicates_exact": final_duplicates_exact,
        }
        return result_df, metadata_dic

    return result_df


def _clean_internal_duplicates(df, id_fields, date_col, date_fmt, policy):
    """Funci√≥n auxiliar para limpiar duplicados internos en un DataFrame."""
    import pandas as pd
    
    if policy in ("keep_newest", "keep_oldest"):
        # Convertir fechas
        df_temp = df.copy()
        df_temp[f'{date_col}_parsed'] = pd.to_datetime(df_temp[date_col], format=date_fmt, errors="coerce")
        
        if policy == "keep_newest":
            df_temp['_sort_date'] = df_temp[f'{date_col}_parsed'].fillna(pd.Timestamp('1900-01-01'))
            df_cleaned = df_temp.sort_values('_sort_date', ascending=False).drop_duplicates(subset=id_fields, keep='first')
        else:  # keep_oldest
            df_temp['_sort_date'] = df_temp[f'{date_col}_parsed'].fillna(pd.Timestamp('2099-12-31'))
            df_cleaned = df_temp.sort_values('_sort_date', ascending=True).drop_duplicates(subset=id_fields, keep='first')
        
        # Limpiar columnas temporales
        df_cleaned = df_cleaned.drop(columns=[f'{date_col}_parsed', '_sort_date'], errors='ignore')
    else:
        # Para keep_df_initial o keep_df_to_merge, mantener el primero
        df_cleaned = df.drop_duplicates(subset=id_fields, keep='first')
    
    return df_cleaned


def _resolve_conflicts(df_conflicts, id_fields, date_col, date_fmt, policy):
    """Funci√≥n auxiliar para resolver conflictos entre DataFrames."""
    import pandas as pd
    
    if policy == "keep_newest":
        df_conflicts[f'{date_col}_parsed'] = pd.to_datetime(df_conflicts[date_col], format=date_fmt, errors="coerce")
        df_conflicts['_sort_date'] = df_conflicts[f'{date_col}_parsed'].fillna(pd.Timestamp('1900-01-01'))
        df_conflicts['_sort_source'] = (df_conflicts['_source'] == 'to_merge').astype(int)
        
        df_sorted = df_conflicts.sort_values(['_sort_date', '_sort_source'], ascending=[False, True])
        resolved = df_sorted.drop_duplicates(subset=id_fields, keep='first')
        
    elif policy == "keep_oldest":
        df_conflicts[f'{date_col}_parsed'] = pd.to_datetime(df_conflicts[date_col], format=date_fmt, errors="coerce")
        df_conflicts['_sort_date'] = df_conflicts[f'{date_col}_parsed'].fillna(pd.Timestamp('2099-12-31'))
        df_conflicts['_sort_source'] = (df_conflicts['_source'] == 'to_merge').astype(int)
        
        df_sorted = df_conflicts.sort_values(['_sort_date', '_sort_source'], ascending=[True, True])
        resolved = df_sorted.drop_duplicates(subset=id_fields, keep='first')
        
    elif policy == "keep_df_initial":
        df_conflicts['_sort_source'] = (df_conflicts['_source'] == 'to_merge').astype(int)
        df_sorted = df_conflicts.sort_values('_sort_source')
        resolved = df_sorted.drop_duplicates(subset=id_fields, keep='first')
        
    elif policy == "keep_df_to_merge":
        df_conflicts['_sort_source'] = (df_conflicts['_source'] == 'initial').astype(int)
        df_sorted = df_conflicts.sort_values('_sort_source')
        resolved = df_sorted.drop_duplicates(subset=id_fields, keep='first')
    
    # Limpiar columnas temporales
    temp_cols = [col for col in resolved.columns if '_parsed' in col or '_sort_' in col]
    resolved = resolved.drop(columns=temp_cols, errors='ignore')
    
    return resolved















# ----------------------------------------------------------------------------
# DType_df_to_df()
# ----------------------------------------------------------------------------
def DType_df_to_df(config: Dict[str, Any]):
    """Copia *dtypes* entre DataFrames con coerci√≥n robusta.

    Claves flexibles admitidas en el `config`:
    - ``reference_dtype_df`` / ``source_df`` ‚Üí DataFrame cuya *signature* de `dtypes` act√∫a de referencia.
    - ``target_dtype_df`` / ``targete_dtype_df`` / ``target_df`` ‚Üí DataFrame al que se le aplicar√°n los dtypes.

    Par√°metros opcionales:
    ---------------------
    inplace : bool  (default ``True``)
        Si *True*, muta el ``target`` *in‚Äëplace*; si *False* trabaja con una copia.
    return_metadata : bool  (default ``True``)
        Devuelve un segundo objeto con informaci√≥n de columnas casteadas / fallidas / omitidas.
    decimal_comma : bool  (default ``True``)
        Pre‚Äëprocesa strings reemplazando `"," ‚Üí "."` antes de la coerci√≥n num√©rica.

    Returns
    -------
    pd.DataFrame | Tuple[pd.DataFrame, dict]
        El DataFrame transformado y, opcionalmente, un diccionario con metadatos.
    """
    import pandas as pd
    import numpy as np
    from typing import Dict, Any, List, Tuple

    # --------------------- VALIDACI√ìN DE ENTRADA ---------------------
    source_df = config.get("reference_dtype_df", config.get("source_df"))
    target_df = config.get("target_dtype_df", config.get("targete_dtype_df", config.get("target_df")))

    if source_df is None or target_df is None:
        raise ValueError("[VALIDATION ‚ùå] Debes proporcionar 'reference_dtype_df/source_df' y 'target_dtype_df/target_df'.")
    if not isinstance(source_df, pd.DataFrame):
        raise ValueError("[VALIDATION ‚ùå] 'reference_dtype_df' no es DataFrame.")
    if not isinstance(target_df, pd.DataFrame):
        raise ValueError("[VALIDATION ‚ùå] 'target_dtype_df' no es DataFrame.")

    inplace: bool = config.get("inplace", True)
    return_metadata: bool = config.get("return_metadata", True)
    decimal_comma: bool = config.get("decimal_comma", True)

    if not inplace:
        target_df = target_df.copy()

    print("üîπüîπüîπ [START ‚ñ∂Ô∏è] DTYPE COPY", flush=True)

    # ------------------------ L√ìGICA PRINCIPAL ------------------------
    common_cols: List[str] = [c for c in source_df.columns if c in target_df.columns]
    if not common_cols:
        print("[DTYPE COPY ‚ö†Ô∏è] No hay columnas coincidentes.", flush=True)
        empty_meta = {"cols_casted": [], "cols_failed": [], "cols_skipped": []}
        return (target_df, empty_meta) if return_metadata else target_df

    cols_casted, cols_failed, cols_skipped = [], [], []

    def _safe_cast(col: pd.Series, tgt_dtype) -> Tuple[pd.Series, bool]:
        """Intenta castear la *Series* a `tgt_dtype`.

        Estrategia:
        1. Intento directo ``astype``.
        2. Si falla y `tgt_dtype` es num√©rico ‚Üí ``pd.to_numeric`` con coerci√≥n, gestionando
           *nullable integer* cuando hay *NaN*.
        3. Si falla y es fecha ‚Üí ``pd.to_datetime``.
        4. *Fallback* ‚Üí string; marca la conversi√≥n como fallida.
        """
        # ‚îÄ‚îÄ Paso 0: pre‚Äëprocesar comas decimales ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        series_proc = col.astype(str).str.replace(",", ".", regex=False) if decimal_comma else col

        # ‚îÄ‚îÄ Paso 1: intento directo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            return col.astype(tgt_dtype), True
        except Exception:
            pass  # continuar√° con coerciones especializadas

        # ‚îÄ‚îÄ Paso 2: coerci√≥n num√©rica ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if pd.api.types.is_numeric_dtype(tgt_dtype):
            coerced = pd.to_numeric(series_proc, errors="coerce")
            if pd.api.types.is_integer_dtype(tgt_dtype):
                # Si existen NaN y el dtype destino es entero ‚áí usar Int64 (nullable)
                if coerced.isna().any():
                    return coerced.astype("Int64"), not coerced.isna().all()
                # sin NaNs: redondea y castea al entero exacto
                return coerced.round().astype(tgt_dtype), True
            # destino float
            return coerced.astype("float64"), not coerced.isna().all()

        # ‚îÄ‚îÄ Paso 3: coerci√≥n de fechas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            if np.issubdtype(tgt_dtype, np.datetime64):
                coerced_dt = pd.to_datetime(series_proc, errors="coerce")
                return coerced_dt, not coerced_dt.isna().all()
        except Exception:
            pass

        # ‚îÄ‚îÄ Paso 4: fallback a string ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        return series_proc.astype(str), False

    total = len(common_cols)
    for idx, col in enumerate(common_cols, start=1):
        tgt_dtype = source_df[col].dtype
        if target_df[col].dtype == tgt_dtype:
            print(f"[DTYPE COPY ‚ÑπÔ∏è] ({idx}/{total}) '{col}' ya es {tgt_dtype}. Skipped.", flush=True)
            cols_skipped.append(col)
            continue

        target_df[col], ok = _safe_cast(target_df[col], tgt_dtype)
        if ok:
            print(f"[DTYPE COPY ‚úÖ] ({idx}/{total}) '{col}' ‚Üí {target_df[col].dtype}.", flush=True)
            cols_casted.append(col)
        else:
            print(f"[DTYPE COPY ‚ö†Ô∏è] ({idx}/{total}) No se pudo castear '{col}' ‚Üí {tgt_dtype}.", flush=True)
            cols_failed.append(col)

    print(
        f"[DTYPE COPY ‚úîÔ∏è] Cast fin ‚Äî ok: {len(cols_casted)}/{total} | "
        f"fail: {len(cols_failed)} | skipped: {len(cols_skipped)}",
        flush=True,
    )

    meta = {
        "cols_casted": cols_casted,
        "cols_failed": cols_failed,
        "cols_skipped": cols_skipped,
    }

    return (target_df, meta) if return_metadata else target_df



















# ----------------------------------------------------------------------------
# table_various_sources_to_DF()
# ----------------------------------------------------------------------------
def table_various_sources_to_DF(params: dict) -> pd.DataFrame:
    """
    Extrae datos desde distintos or√≠genes (archivo, Google Sheets, BigQuery o GCS) y los convierte en un DataFrame.
    
    Par√°metros en params:
      - (ver docstring completo en la versi√≥n original)
    
    Retorna:
      pd.DataFrame: DataFrame con los datos extra√≠dos y procesados.
    
    Raises:
      RuntimeError: Si ocurre un error al extraer o procesar los datos.
      ValueError: Si faltan par√°metros obligatorios para identificar el origen de datos.
    """
    import re
    import io
    import time
    import pandas as pd

    # Para Google Sheets y otros servicios de Google
    import gspread
    try:
        from google.colab import files
    except ImportError:
        pass

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Utilidades Comunes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _imprimir_encabezado(mensaje: str) -> None:
        print(f"\nüîπüîπüîπ {mensaje} üîπüîπüîπ\n", flush=True)

    def _validar_comun(params: dict) -> None:
        if not (params.get('json_keyfile_GCP_secret_id') or params.get('json_keyfile_colab')):
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta el par√°metro obligatorio 'json_keyfile_GCP_secret_id' o 'json_keyfile_colab' para autenticaci√≥n.")

    def _apply_common_filters(df: pd.DataFrame, params: dict) -> pd.DataFrame:
        # Filtrado de filas (si corresponde)
        row_start = params.get('source_table_row_start', 0)
        row_end = params.get('source_table_row_end', None)
        if row_end is not None:
            df = df.iloc[row_start:row_end]
        else:
            df = df.iloc[row_start:]
        
        # Filtrado de columnas por posici√≥n
        col_start = params.get('source_table_col_start', 0)
        col_end = params.get('source_table_col_end', None)
        if col_end is not None:
            df = df.iloc[:, col_start:col_end]
        else:
            df = df.iloc[:, col_start:]
        
        # Selecci√≥n de campos espec√≠ficos si se indica
        if 'source_table_fields_list' in params:
            fields = params['source_table_fields_list']
            fields = [f for f in fields if f in df.columns]
            if fields:
                df = df[fields]
        return df

    def _auto_convert(df: pd.DataFrame) -> pd.DataFrame:
        for col in df.columns:
            col_lower = col.lower()
            if "fecha" in col_lower or col_lower == "valor":
                try:
                    df[col] = pd.to_datetime(df[col], dayfirst=True, errors='coerce')
                except Exception as e:
                    print(f"[TRANSFORMATION [WARNING ‚ö†Ô∏è]] Error al convertir la columna '{col}' a datetime: {e}", flush=True)
            elif col_lower in ['importe', 'saldo']:
                try:
                    df[col] = df[col].apply(lambda x: float(x.replace('.', '').replace(',', '.')) if isinstance(x, str) and x.strip() != '' else x)
                except Exception as e:
                    print(f"[TRANSFORMATION [WARNING ‚ö†Ô∏è]] Error al convertir la columna '{col}' a float: {e}", flush=True)
        return df

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Detecci√≥n del Origen ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _es_fuente_archivo(params: dict) -> bool:
        return bool(params.get('file_source_table_path', '').strip())

    def _es_fuente_gsheet(params: dict) -> bool:
        return (not _es_fuente_archivo(params)) and (
            bool(params.get('spreadsheet_source_table_id', '').strip()) and 
            bool(params.get('spreadsheet_source_table_worksheet_name', '').strip())
        )

    def _es_fuente_gbq(params: dict) -> bool:
        return bool(params.get('GBQ_source_table_name', '').strip())

    def _es_fuente_gcs(params: dict) -> bool:
        return bool(params.get('GCS_source_table_bucket_name', '').strip()) and bool(params.get('GCS_source_table_file_path', '').strip())

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Fuente ‚Äì Archivo Local ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _leer_archivo(params: dict) -> pd.DataFrame:
        _imprimir_encabezado("[START üöÄ] Iniciando carga del archivo")
        file_path = params.get('file_source_table_path')
        row_skip_empty = params.get('source_table_filter_skip_row_empty_use', True)
        row_start = params.get('source_table_row_start', 0)
        row_end = params.get('source_table_row_end', None)
        nrows = (row_end - row_start) if row_end is not None else None
        col_start = params.get('source_table_col_start', 0)
        col_end = params.get('source_table_col_end', None)

        if not file_path:
            print("[EXTRACTION [WARNING ‚ö†Ô∏è]] No se proporcion√≥ 'file_source_table_path'. Suba un archivo desde su ordenador:", flush=True)
            uploaded = files.upload()
            file_path = list(uploaded.keys())[0]
            file_input = io.BytesIO(uploaded[file_path])
            print(f"[EXTRACTION [SUCCESS ‚úÖ]] Archivo '{file_path}' subido exitosamente.", flush=True)
        else:
            file_input = file_path

        _, ext = os.path.splitext(file_path)
        ext = ext.lower()

        try:
            print(f"[EXTRACTION [START ‚è≥]] Leyendo archivo '{file_path}'...", flush=True)
            if ext in ['.xls', '.xlsx']:
                engine = 'xlrd' if ext == '.xls' else 'openpyxl'
                df = pd.read_excel(file_input, engine=engine, skiprows=row_start, nrows=nrows)
            elif ext == '.csv':
                df = pd.read_csv(file_input, skiprows=row_start, nrows=nrows, sep=',')
            elif ext == '.tsv':
                df = pd.read_csv(file_input, skiprows=row_start, nrows=nrows, sep='\t')
            else:
                raise RuntimeError(f"[EXTRACTION [ERROR ‚ùå]] Extensi√≥n de archivo '{ext}' no soportada.")
            
            if col_end is not None:
                df = df.iloc[:, col_start:col_end]
            else:
                df = df.iloc[:, col_start:]
            
            if row_skip_empty:
                initial_rows = len(df)
                df.dropna(how='all', inplace=True)
                removed_rows = initial_rows - len(df)
                print(f"[TRANSFORMATION [SUCCESS ‚úÖ]] Se eliminaron {removed_rows} filas vac√≠as.", flush=True)
            
            df = df.convert_dtypes()
            df = _auto_convert(df)

            print("\n[METRICS [INFO üìä]] INFORME ESTAD√çSTICO DEL DATAFRAME:")
            print(f"  - Total filas: {df.shape[0]}")
            print(f"  - Total columnas: {df.shape[1]}")
            print("  - Tipos de datos por columna:")
            print(df.dtypes)
            print("  - Resumen estad√≠stico (num√©rico):")
            print(df.describe())
            print("  - Resumen estad√≠stico (incluyendo variables categ√≥ricas):")
            print(df.describe(include='all'))
            print(f"\n[END [FINISHED üèÅ]] Archivo '{file_path}' cargado correctamente. Filas: {df.shape[0]}, Columnas: {df.shape[1]}", flush=True)
            return df

        except Exception as e:
            error_message = f"[EXTRACTION [ERROR ‚ùå]] Error al leer el archivo '{file_path}': {e}"
            print(error_message, flush=True)
            raise RuntimeError(error_message)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Fuente ‚Äì Google Sheets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _leer_google_sheet(params: dict) -> pd.DataFrame:
        from googleapiclient.discovery import build

        spreadsheet_id_raw = params.get("spreadsheet_source_table_id")
        if "spreadsheets/d/" in spreadsheet_id_raw:
            match = re.search(r"/d/([a-zA-Z0-9-_]+)", spreadsheet_id_raw)
            if match:
                spreadsheet_id = match.group(1)
            else:
                raise ValueError("[VALIDATION [ERROR ‚ùå]] No se pudo extraer el ID de la hoja de c√°lculo desde la URL proporcionada.")
        else:
            spreadsheet_id = spreadsheet_id_raw

        worksheet_name = params.get("spreadsheet_source_table_worksheet_name")
        if not spreadsheet_id or not worksheet_name:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Faltan 'spreadsheet_source_table_id' o 'spreadsheet_source_table_worksheet_name'.")

        try:
            scope_list = [
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive"
            ]
            # Determinaci√≥n del project_id seg√∫n el entorno
            ini_env = params.get("ini_environment_identificated")
            if not ini_env:
                raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta la key 'ini_environment_identificated' en params.")
            project_id = os.environ.get("GOOGLE_CLOUD_PROJECT") if ini_env == "COLAB_ENTERPRISE" else ini_env

            # Uso de _ini_authenticate_API para la autenticaci√≥n
            creds = _ini_authenticate_API(params, project_id)
            creds = creds.with_scopes(scope_list)
            
            service = build('sheets', 'v4', credentials=creds)
            range_name = f"{worksheet_name}"
            print("[EXTRACTION [START ‚è≥]] Extrayendo datos de Google Sheets...", flush=True)
            result = service.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()
            data = result.get('values', [])
            if not data:
                print("[EXTRACTION [WARNING ‚ö†Ô∏è]] No se encontraron datos en la hoja especificada.", flush=True)
                return pd.DataFrame()
            
            # Se obtiene el encabezado y se ajustan las filas para que todas tengan la misma longitud.
            header = data[0]
            n_columns = len(header)
            data_fixed = []
            for row in data[1:]:
                if len(row) < n_columns:
                    row = row + [None] * (n_columns - len(row))
                elif len(row) > n_columns:
                    row = row[:n_columns]
                data_fixed.append(row)

            df = pd.DataFrame(data_fixed, columns=header)
            print(f"[EXTRACTION [SUCCESS ‚úÖ]] Datos extra√≠dos con √©xito de la hoja '{worksheet_name}'.", flush=True)
            return df

        except Exception as e:
            raise ValueError(f"[EXTRACTION [ERROR ‚ùå]] Error al extraer datos de Google Sheets: {e}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Fuente ‚Äì BigQuery ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _leer_gbq(params: dict) -> pd.DataFrame:
        scope_list = ["https://www.googleapis.com/auth/bigquery", "https://www.googleapis.com/auth/drive"]
        ini_env = params.get("ini_environment_identificated")
        if not ini_env:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta la key 'ini_environment_identificated' en params.")
        project_id = os.environ.get("GOOGLE_CLOUD_PROJECT") if ini_env == "COLAB_ENTERPRISE" else ini_env

        # Uso de _ini_authenticate_API para la autenticaci√≥n en BigQuery
        from google.oauth2.service_account import Credentials
        creds = _ini_authenticate_API(params, project_id)
        creds = creds.with_scopes(scope_list)
    
        client_bq = bigquery.Client(credentials=creds, project=project_id)
        gbq_table = params.get("GBQ_source_table_name")
        if not gbq_table:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta el par√°metro 'GBQ_source_table_name' para BigQuery.")
        try:
            query = f"SELECT * FROM `{gbq_table}`"
            print(f"[EXTRACTION [START ‚è≥]] Ejecutando consulta en BigQuery: {query}", flush=True)
            df = client_bq.query(query).to_dataframe()
            print("[EXTRACTION [SUCCESS ‚úÖ]] Datos extra√≠dos con √©xito de BigQuery.", flush=True)
            return df
        except Exception as e:
            raise RuntimeError(f"[EXTRACTION [ERROR ‚ùå]] Error al extraer datos de BigQuery: {e}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Fuente ‚Äì Google Cloud Storage (GCS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _leer_gcs(params: dict) -> pd.DataFrame:
        scope_list = ["https://www.googleapis.com/auth/devstorage.read_only"]
        ini_env = params.get("ini_environment_identificated")
        if not ini_env:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta la key 'ini_environment_identificated' en params.")
        project_id = os.environ.get("GOOGLE_CLOUD_PROJECT") if ini_env == "COLAB_ENTERPRISE" else ini_env

        # Uso de _ini_authenticate_API para la autenticaci√≥n en GCS
        from google.oauth2.service_account import Credentials
        creds = _ini_authenticate_API(params, project_id)
        creds = creds.with_scopes(scope_list)
    
        try:
            bucket_name = params.get("GCS_source_table_bucket_name")
            file_path = params.get("GCS_source_table_file_path")
            if not bucket_name or not file_path:
                raise ValueError("[VALIDATION [ERROR ‚ùå]] Faltan 'GCS_source_table_bucket_name' o 'GCS_source_table_file_path'.")
            from google.cloud import storage
            client_storage = storage.Client(credentials=creds, project=project_id)
            bucket = client_storage.bucket(bucket_name)
            blob = bucket.blob(file_path)
            print(f"[EXTRACTION [START ‚è≥]] Descargando archivo '{file_path}' del bucket '{bucket_name}'...", flush=True)
            file_bytes = blob.download_as_bytes()
            _, ext = os.path.splitext(file_path)
            ext = ext.lower()
            if ext in ['.xls', '.xlsx']:
                engine = 'xlrd' if ext == '.xls' else 'openpyxl'
                df = pd.read_excel(io.BytesIO(file_bytes), engine=engine)
            elif ext == '.csv':
                df = pd.read_csv(io.BytesIO(file_bytes), sep=',')
            elif ext == '.tsv':
                df = pd.read_csv(io.BytesIO(file_bytes), sep='\t')
            else:
                raise RuntimeError(f"[EXTRACTION [ERROR ‚ùå]] Extensi√≥n de archivo '{ext}' no soportada en GCS.")
            print("[EXTRACTION [SUCCESS ‚úÖ]] Archivo descargado y le√≠do desde GCS.", flush=True)
            return df
        except Exception as e:
            raise RuntimeError(f"[EXTRACTION [ERROR ‚ùå]] Error al leer archivo desde GCS: {e}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PROCESO PRINCIPAL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    _validar_comun(params)
    if _es_fuente_archivo(params):
        df = _leer_archivo(params)
    elif _es_fuente_gsheet(params):
        df = _leer_google_sheet(params)
    elif _es_fuente_gbq(params):
        df = _leer_gbq(params)
    elif _es_fuente_gcs(params):
        df = _leer_gcs(params)
    else:
        raise ValueError(
            "[VALIDATION [ERROR ‚ùå]] No se han proporcionado par√°metros v√°lidos para identificar el origen de datos. "
            "Defina 'file_source_table_path', 'spreadsheet_source_table_id' y 'spreadsheet_source_table_worksheet_name', "
            "'GBQ_source_table_name' o 'GCS_source_table_bucket_name' y 'GCS_source_table_file_path'."
        )

    df = _apply_common_filters(df, params)
    return df




















def table_DF_to_various_targets(params: dict) -> None:
    """
    Escribe un DataFrame en distintos destinos (archivo local, Google Sheets,
    BigQuery o GCS) seg√∫n la configuraci√≥n definida en el diccionario de
    entrada, permitiendo especificar el modo de escritura: sobrescribir
    ('overwrite') o agregar ('append').

    Args
    ----
      params (dict):
        - df (pd.DataFrame): DataFrame a exportar.
        - ini_environment_identificated (str): 'LOCAL', 'COLAB', 'COLAB_ENTERPRISE'
          o el ID del proyecto GCP.
        - Claves de autenticaci√≥n (una de ellas seg√∫n el entorno):
            ¬∑ json_keyfile_local
            ¬∑ json_keyfile_colab
            ¬∑ json_keyfile_GCP_secret_id
        - Par√°metros de destino (uno de los grupos):
            ¬∑ file_target_table_path, file_target_table_overwrite_or_append
            ¬∑ spreadsheet_target_table_id,
              spreadsheet_target_table_worksheet_name,
              spreadsheet_target_table_overwrite_or_append
            ¬∑ GBQ_target_table_name, GBQ_target_table_overwrite_or_append
            ¬∑ GCS_target_table_bucket_name, GCS_target_table_file_path,
              GCS_target_table_overwrite_or_append

    Raises
    ------
      ValueError  : Si faltan par√°metros obligatorios o son inv√°lidos.
      RuntimeError: Si ocurre un error durante la escritura.
    """
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ IMPORTS B√ÅSICOS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    import os, io, math
    import pandas as pd, numpy as np
    from google.cloud import bigquery
    from google.oauth2.service_account import Credentials

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ VALIDACIONES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print("\nüîπüîπüîπ [START ‚ñ∂Ô∏è] Iniciando escritura de DataFrame en destino configurado üîπüîπüîπ\n", flush=True)

    df = params.get("df")
    if df is None or not isinstance(df, pd.DataFrame):
        raise ValueError("[VALIDATION [ERROR ‚ùå]] La clave 'df' debe contener un DataFrame v√°lido.")
    print(f"[METRICS [INFO ‚ÑπÔ∏è]] DataFrame recibido: {df.shape[0]} filas √ó {df.shape[1]} columnas.", flush=True)

    if not any(params.get(k) for k in ("json_keyfile_GCP_secret_id", "json_keyfile_colab", "json_keyfile_local")):
        raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta un par√°metro de keyfile para autenticaci√≥n.")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DETECCI√ìN DEL DESTINO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    _es_target_archivo = lambda p: bool(p.get('file_target_table_path', '').strip())
    _es_target_gsheet  = lambda p: not _es_target_archivo(p) and \
                                   bool(p.get('spreadsheet_target_table_id', '').strip()) and \
                                   bool(p.get('spreadsheet_target_table_worksheet_name', '').strip())
    _es_target_gbq     = lambda p: bool(p.get('GBQ_target_table_name', '').strip())
    _es_target_gcs     = lambda p: bool(p.get('GCS_target_table_bucket_name', '').strip()) and \
                                   bool(p.get('GCS_target_table_file_path', '').strip())

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SUB-FUNCIONES DE ESCRITURA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _escribir_archivo(p: dict, d: pd.DataFrame) -> None:
        import os
        print("\n[LOAD [START ‚ñ∂Ô∏è]] Iniciando escritura en archivo local‚Ä¶", flush=True)
        mode  = p.get("file_target_table_overwrite_or_append", "overwrite").lower()
        fpath = p.get("file_target_table_path")
        if not fpath:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta 'file_target_table_path'.")
        _, ext = os.path.splitext(fpath); ext = ext.lower()

        try:
            if ext in {'.xls', '.xlsx'}:
                engine = 'openpyxl'
                if mode == "append" and os.path.exists(fpath):
                    d = pd.concat([pd.read_excel(fpath, engine=engine), d], ignore_index=True)
                d.to_excel(fpath, index=False, engine=engine)

            elif ext in {'.csv', '.tsv'}:
                sep    = '\t' if ext == '.tsv' else ','
                header = not (mode == "append" and os.path.exists(fpath))
                d.to_csv(fpath, sep=sep, index=False,
                         mode=('a' if mode == "append" else 'w'),
                         header=header)
            else:
                raise RuntimeError(f"Extensi√≥n '{ext}' no soportada para archivos locales.")

            print(f"[LOAD [SUCCESS ‚úÖ]] DataFrame escrito en '{fpath}'.", flush=True)
            print(f"[METRICS [INFO ‚ÑπÔ∏è]] Destino final: file://{fpath}", flush=True)
        except Exception as e:
            raise RuntimeError(f"[LOAD [ERROR ‚ùå]] Error al escribir en archivo local: {e}")

    def _escribir_google_sheet(p: dict, d: pd.DataFrame) -> None:
        """
        Env√≠a el DataFrame a Google Sheets respetando los formatos de fecha-hora
        espa√±oles (coma decimal). Se sobrescribe o a√±ade seg√∫n `mode`.
        """
        import re
        from googleapiclient.discovery import build
        print("\n[LOAD [START ‚ñ∂Ô∏è]] Iniciando escritura en Google Sheets‚Ä¶", flush=True)

        mode = p.get("spreadsheet_target_table_overwrite_or_append", "overwrite").lower()
        raw_id   = p.get("spreadsheet_target_table_id")
        ws_name  = p.get("spreadsheet_target_table_worksheet_name")
        if not raw_id or not ws_name:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Faltan 'spreadsheet_target_table_id' o 'spreadsheet_target_table_worksheet_name'.")

        # Normalizar el ID (por si viene una URL completa)
        m = re.search(r"/d/([A-Za-z0-9-_]+)", raw_id)
        sheet_id = m.group(1) if m else raw_id

        scopes = ["https://www.googleapis.com/auth/spreadsheets",
                  "https://www.googleapis.com/auth/drive"]
        env = p.get("ini_environment_identificated")
        project = os.getenv("GOOGLE_CLOUD_PROJECT") if env == "COLAB_ENTERPRISE" else env
        creds   = _ini_authenticate_API(params, project).with_scopes(scopes)
        service = build('sheets', 'v4', credentials=creds)

        # ‚îÄ‚îÄ NUEVO: convertir columnas datetime a texto con coma decimal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        datetime_cols = d.select_dtypes(include=["datetime64[ns]", "datetime64[ns, utc]"]).columns
        if len(datetime_cols) > 0:
            d = d.copy()  # evitamos SettingWithCopyWarning
            for col in datetime_cols:
                # Ej.: 2021-02-09 08:52:29,217577  (coma decimal)
                d[col] = d[col].dt.strftime("%Y-%m-%d %H:%M:%S,%f")
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        # ‚îÄ‚îÄ Conversi√≥n de cada celda ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        from decimal import Decimal, InvalidOperation
        def _cast(value):
            """
            Devuelve un n√∫mero nativo, None o str.
            - Timestamps ya vienen formateados como str con coma.
            - Si prefieres coma decimal en todos los floats, cambia aqu√≠ `float(value)`
              por `str(value).replace('.', ',')`.
            """
            if pd.isna(value):
                return None
            if isinstance(value, (float, np.floating, Decimal)):
                try:
                    return float(value)
                except (ValueError, InvalidOperation):
                    return str(value)
            if isinstance(value, (int, np.integer)):
                return int(value)
            return str(value)

        rows = [[_cast(val) for val in row] for row in d.itertuples(index=False, name=None)]
        values = ([d.columns.tolist()] + rows) if mode == "overwrite" else rows

        # Limpiar rango si es overwrite
        if mode == "overwrite":
            service.spreadsheets().values().clear(
                spreadsheetId=sheet_id,
                range=ws_name,
                body={}
            ).execute()

        request = (service.spreadsheets().values().update if mode == "overwrite"
                   else service.spreadsheets().values().append)
        result = request(
            spreadsheetId   = sheet_id,
            range           = ws_name,
            valueInputOption= "RAW",
            body            = {"values": values},
            **({"insertDataOption": "INSERT_ROWS"} if mode == "append" else {})
        ).execute()

        affected = result.get('updates', {}).get('updatedCells', 'N/A')
        print(f"[LOAD [SUCCESS ‚úÖ]] Se actualizaron {affected} celdas en Google Sheets.", flush=True)
        print(f"[METRICS [INFO ‚ÑπÔ∏è]] Destino final: https://docs.google.com/spreadsheets/d/{sheet_id}", flush=True)

    def _escribir_gbq(p: dict, d: pd.DataFrame) -> None:
        from google.cloud.bigquery import LoadJobConfig, WriteDisposition
        print("\n[LOAD [START ‚ñ∂Ô∏è]] Iniciando carga en BigQuery‚Ä¶", flush=True)

        mode  = p.get("GBQ_target_table_overwrite_or_append", "overwrite").lower()
        table = p.get("GBQ_target_table_name")
        if not table:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Falta 'GBQ_target_table_name'.")

        env = p.get("ini_environment_identificated")
        project = os.getenv("GOOGLE_CLOUD_PROJECT") if env == "COLAB_ENTERPRISE" else env
        creds = _ini_authenticate_API(params, project).with_scopes(
            ["https://www.googleapis.com/auth/bigquery",
             "https://www.googleapis.com/auth/drive"])
        client = bigquery.Client(credentials=creds, project=project)

        job_cfg = LoadJobConfig(
            write_disposition=(WriteDisposition.WRITE_TRUNCATE if mode == "overwrite"
                               else WriteDisposition.WRITE_APPEND)
        )
        try:
            client.load_table_from_dataframe(d, table, job_config=job_cfg).result()
            print("[LOAD [SUCCESS ‚úÖ]] DataFrame cargado exitosamente en BigQuery.", flush=True)
            print(f"[METRICS [INFO ‚ÑπÔ∏è]] Destino final: https://console.cloud.google.com/bigquery?project={project}&ws=!1m5!1m4!4m3!1s{table}", flush=True)
        except Exception as e:
            raise RuntimeError(f"[LOAD [ERROR ‚ùå]] Error al escribir en BigQuery: {e}")

    def _escribir_gcs(p: dict, d: pd.DataFrame) -> None:
        from google.cloud import storage
        print("\n[LOAD [START ‚ñ∂Ô∏è]] Iniciando subida a Google Cloud Storage‚Ä¶", flush=True)

        bucket = p.get("GCS_target_table_bucket_name")
        path   = p.get("GCS_target_table_file_path")
        if not bucket or not path:
            raise ValueError("[VALIDATION [ERROR ‚ùå]] Faltan 'GCS_target_table_bucket_name' o 'GCS_target_table_file_path'.")

        mode = p.get("GCS_target_table_overwrite_or_append", "overwrite").lower()
        env  = p.get("ini_environment_identificated")
        project = os.getenv("GOOGLE_CLOUD_PROJECT") if env == "COLAB_ENTERPRISE" else env
        creds   = _ini_authenticate_API(params, project).with_scopes(
            ["https://www.googleapis.com/auth/devstorage.read_write"])
        client  = storage.Client(credentials=creds, project=project)
        blob    = client.bucket(bucket).blob(path)
        _, ext  = os.path.splitext(path); ext = ext.lower()

        try:
            # Si append, descargar y concatenar
            if mode == "append" and blob.exists(client):
                data = blob.download_as_bytes()
                if ext == '.csv':
                    d = pd.concat([pd.read_csv(io.BytesIO(data)), d], ignore_index=True)
                elif ext == '.tsv':
                    d = pd.concat([pd.read_csv(io.BytesIO(data), sep='\t'), d], ignore_index=True)
                elif ext in {'.xls', '.xlsx'}:
                    d = pd.concat([pd.read_excel(io.BytesIO(data)), d], ignore_index=True)
                else:
                    raise RuntimeError(f"Extensi√≥n '{ext}' no soportada en modo append.")

            # Serializar y subir
            if ext in {'.xls', '.xlsx'}:
                buf = io.BytesIO(); d.to_excel(buf, index=False, engine='openpyxl')
                blob.upload_from_string(buf.getvalue(),
                                        content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
            elif ext == '.csv':
                blob.upload_from_string(d.to_csv(index=False).encode('utf-8'), content_type='text/csv')
            elif ext == '.tsv':
                blob.upload_from_string(d.to_csv(sep='\t', index=False).encode('utf-8'),
                                        content_type='text/tab-separated-values')
            else:
                raise RuntimeError(f"Extensi√≥n '{ext}' no soportada para GCS.")

            print("[LOAD [SUCCESS ‚úÖ]] Archivo subido exitosamente a GCS.", flush=True)
            print(f"[METRICS [INFO ‚ÑπÔ∏è]] Destino final: https://console.cloud.google.com/storage/browser/{bucket}?project={project}", flush=True)
        except Exception as e:
            raise RuntimeError(f"[LOAD [ERROR ‚ùå]] Error al escribir en GCS: {e}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DESPACHADOR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        if _es_target_archivo(params):
            _escribir_archivo(params, df)
        elif _es_target_gsheet(params):
            _escribir_google_sheet(params, df)
        elif _es_target_gbq(params):
            _escribir_gbq(params, df)
        elif _es_target_gcs(params):
            _escribir_gcs(params, df)
        else:
            raise ValueError(
                "[VALIDATION [ERROR ‚ùå]] No se detect√≥ un destino v√°lido. "
                "Defina 'file_target_table_path', "
                "o ('spreadsheet_target_table_id' y 'spreadsheet_target_table_worksheet_name'), "
                "o 'GBQ_target_table_name', "
                "o ('GCS_target_table_bucket_name' y 'GCS_target_table_file_path')."
            )
    except Exception as e:
        print(f"\nüîπüîπüîπ [END [FAILED ‚ùå]] Proceso finalizado con errores: {e} üîπüîπüîπ\n", flush=True)
        raise

    print("\nüîπüîπüîπ [END [FINISHED ‚úÖ]] Escritura completada exitosamente. üîπüîπüîπ\n", flush=True)
